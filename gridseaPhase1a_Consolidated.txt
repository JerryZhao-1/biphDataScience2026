============================================================
Cell 0 [markdown]
============================================================
# Phase 1a: Consolidated WHL Hockey Ranking & Prediction Pipeline

**Objective**: End-to-end pipeline from raw shift-level data ‚Üí Elo power rankings ‚Üí matchup predictions.

| Step | Description |
|------|-------------|
| 1 | Data Cleaning & Aggregation (shift ‚Üí game level) ‚Äî **keep empty-net records** |
| 2 | Restructuring to Long Format (game ‚Üí team perspective) |
| 3 | Weighting Strategy (MoV multiplier + xG performance score) |
| 4 | Monte Carlo Elo Simulation (bagged ratings) |
| 5 | Cross-Validation & HFA Calibration |
| 6 | Pythagorean Expectation, PDO Luck Check, and Final Predictions |

### Key Design Decisions
- **Empty-net rows are KEPT**: removing them changes the winner of 62 games and creates 57 false draws
- **HFA calibrated from data**: empirical home win rate = 56.4% ‚Üí HFA ‚âà 45‚Äì50 Elo points
- **MoV capped** to prevent blowout games from dominating Elo updates
- **Penalties data** incorporated as special-teams efficiency proxy

============================================================
Cell 1 [markdown]
============================================================
---
## 1. Imports & Configuration

============================================================
Cell 2 [code]
============================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
from tqdm import tqdm
from sklearn.metrics import log_loss

# ‚îÄ‚îÄ‚îÄ Reproducibility ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
np.random.seed(42)
random.seed(42)

# ‚îÄ‚îÄ‚îÄ Global Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# All tuneable parameters live here ‚Äî never buried in code.

DATA_PATH          = 'd:/A/Warton/Data/whl_2025.csv'    # Raw shift-level data
MATCHUP_PATH       = 'd:/A/Warton/Data/matchups.csv'    # Round 1 matchups
SUBMISSION_PATH    = 'd:/A/Warton/Data/submission.csv'   # Output predictions

# Elo Engine Parameters
N_SIMULATIONS      = 1000       # Monte Carlo iterations (1000 for stable CI)
K_FACTOR           = 20         # Elo update sensitivity (lower = more stable)
HOME_ADVANTAGE     = 15         # Home-ice advantage in Elo points (data-calibrated)
ELO_BASE           = 1500.0     # Starting rating for all teams
ELO_SCALE          = 400        # Denominator in the logistic Elo formula

# Weighting Parameters for S_actual = W_RESULT * result_score + W_XG * xg_share
# Result should dominate (it's what we're predicting), xG is a stabilizer
WEIGHT_RESULT      = 0.4        # Weight on normalized result (points/3)
WEIGHT_XG          = 0.6        # Weight on xG process quality
MOV_LOG_BASE       = 1          # Offset in ln(|GD| + base)
MOV_CAP            = 1.5        # Cap MoV multiplier to limit blowout influence

# Pythagorean Expectation exponent (hockey empirical value ~ 2.0-2.15)
PYTH_EXPONENT      = 2.15

# Points System: Regulation Win=3, OT Win=2, OT Loss=1, Regulation Loss=0
PTS_REG_WIN  = 3
PTS_OT_WIN   = 2
PTS_OT_LOSS  = 1
PTS_REG_LOSS = 0

# PDO sustainability threshold (above this -> "lucky")
PDO_THRESHOLD      = 1.04
PDO_TOP_N          = 5          # Check top-N teams for unsustainable PDO

print("Configuration loaded.")
print(f"   Simulations: {N_SIMULATIONS} | K: {K_FACTOR} | HFA: {HOME_ADVANTAGE}")
print(f"   Weights: Result={WEIGHT_RESULT} / xG={WEIGHT_XG} | MoV cap: {MOV_CAP}")

============================================================
Cell 3 [markdown]
============================================================
---
## 2. Helper Functions (Math & Logic)

Core Elo mathematics ‚Äî defined **once**, reused everywhere.

============================================================
Cell 4 [code]
============================================================
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  Elo Mathematical Primitives
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def elo_expected_score(rating_a: float, rating_b: float, hfa: float = 0.0) -> float:
    """
    Logistic expected score for Team A against Team B.
    
    Based on the Bradley-Terry model:
        E(A) = 1 / (1 + 10^((R_B - (R_A + HFA)) / 400))
    
    Parameters
    ----------
    rating_a : float - Current Elo rating of Team A.
    rating_b : float - Current Elo rating of Team B.
    hfa      : float - Home-field advantage bonus added to Team A's rating.
    
    Returns
    -------
    float - Expected probability of Team A winning (0.0 to 1.0).
    """
    return 1.0 / (1.0 + 10.0 ** ((rating_b - (rating_a + hfa)) / ELO_SCALE))


def margin_of_victory_multiplier(goal_diff: int) -> float:
    """
    Logarithmic Margin-of-Victory multiplier, CAPPED to prevent blowouts
    from dominating rating updates.
    
        M = min(ln(|GD| + 1), MOV_CAP)
    
    Without the cap, a 7-0 game (M=2.08) has 3x the influence of a 1-0
    game (M=0.69). Capping at 1.5 limits this to ~2.2x.
    
    Parameters
    ----------
    goal_diff : int - Absolute goal differential.
    
    Returns
    -------
    float - Multiplier >= 0, capped at MOV_CAP.
    """
    return min(np.log(abs(goal_diff) + MOV_LOG_BASE), MOV_CAP)


def composite_actual_score(result_points: int, xg_share: float) -> float:
    """
    Blend result with process-based (xG) performance.
    
    CHANGED from previous version:
    - Uses normalized points (0/3=0.0, 1/3=0.33, 2/3=0.67, 3/3=1.0)
      instead of binary win/loss, preserving OT information
    - Result weight increased to 0.6 (was effectively 0.12 before)
    - xG weight decreased to 0.4 (was effectively 0.42 before)
    
        S_actual = W_RESULT * (points/3) + W_XG * xg_share
    
    Parameters
    ----------
    result_points : int   - League points earned (0, 1, 2, or 3).
    xg_share      : float - xG share (xGF / (xGF + xGA)), 0.0 to 1.0.
    
    Returns
    -------
    float - Blended actual score.
    """
    result_normalized = result_points / 3.0  # 0, 0.33, 0.67, or 1.0
    return WEIGHT_RESULT * result_normalized + WEIGHT_XG * xg_share


def update_elo_ratings(
    rating_home: float,
    rating_away: float,
    result_pts_home: int,
    xg_share_home: float,
    xg_share_away: float,
    mov_mult: float
) -> tuple:
    """
    Full Elo update step for a single game.
    
    Update rule:
        R_new = R_old + K * M * (S_actual - E)
    
    The S_actual values are normalized to sum to 1.0 so that
    the system remains zero-sum (total Elo across all teams is constant).
    
    Parameters
    ----------
    rating_home     : float - Home team current Elo.
    rating_away     : float - Away team current Elo.
    result_pts_home : int   - Home team league points (0-3).
    xg_share_home   : float - Home team xG share.
    xg_share_away   : float - Away team xG share.
    mov_mult        : float - Margin-of-victory multiplier.
    
    Returns
    -------
    (float, float) - Updated (home_rating, away_rating).
    """
    # Expected scores (logistic model with home-ice advantage)
    exp_home = elo_expected_score(rating_home, rating_away, hfa=HOME_ADVANTAGE)
    exp_away = 1.0 - exp_home

    # Derive away result from home result (zero-sum league points)
    pts_map = {3: 0, 2: 1, 1: 2, 0: 3}
    result_pts_away = pts_map.get(result_pts_home, 0)

    # Composite actual scores
    s_home = composite_actual_score(result_pts_home, xg_share_home)
    s_away = composite_actual_score(result_pts_away, xg_share_away)

    # Normalize to enforce zero-sum Elo conservation
    total = s_home + s_away
    if total == 0:
        s_home, s_away = 0.5, 0.5
    else:
        s_home /= total
        s_away /= total

    # Elo update: R_new = R_old + K * MoV * (S_actual - E)
    new_home = rating_home + K_FACTOR * mov_mult * (s_home - exp_home)
    new_away = rating_away + K_FACTOR * mov_mult * (s_away - exp_away)

    return new_home, new_away


def classify_risk(prob_home: float) -> str:
    """Categorize prediction confidence."""
    if abs(prob_home - 0.5) < 0.05:
        return 'High Risk / Potential OT'
    elif prob_home > 0.65 or prob_home < 0.35:
        return 'Solid Bet'
    else:
        return 'Medium Risk'


print("Helper functions defined.")

============================================================
Cell 5 [markdown]
============================================================
---
## 3. Data Pipeline: Preprocessing (Steps 1-3)

Three pipeline functions transform raw CSV -> analysis-ready DataFrame.

**Critical change**: Empty-net records are **kept** during aggregation. 
Removing them changed the winner of 62 games and created 57 false draws 
(because empty-net goals are real goals that determine the final score).

============================================================
Cell 6 [code]
============================================================
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  Step 1: Raw Data -> Game-level Aggregation
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def load_and_clean(filepath: str) -> pd.DataFrame:
    """
    Load raw shift-level CSV and remove duplicates.
    
    IMPORTANT: Empty-net records are KEPT.
    They contain real goals that determine game outcomes.
    Removing them changes the winner of 62/1312 games.
    
    Returns
    -------
    pd.DataFrame - Cleaned shift-level data.
    """
    df_raw = pd.read_csv(filepath)
    n_original = len(df_raw)
    
    # 1) Remove duplicate records
    if 'record_id' in df_raw.columns:
        dup_mask = df_raw.duplicated(subset=['record_id'])
    else:
        dup_mask = df_raw.duplicated()
    n_dups = int(dup_mask.sum())
    df_raw = df_raw.loc[~dup_mask].copy()
    
    # 2) Tag empty-net rows (for optional separate analysis) but DO NOT remove
    empty_net_cols = [
        'home_goalie', 'away_goalie',
        'home_off_line', 'away_off_line',
        'home_def_pairing', 'away_def_pairing'
    ]
    en_mask = pd.Series(False, index=df_raw.index)
    for col in empty_net_cols:
        if col in df_raw.columns:
            en_mask |= df_raw[col].astype(str).str.contains('empty_net', case=False, na=False)
    n_empty_net = int(en_mask.sum())
    df_raw['is_empty_net'] = en_mask.astype(int)
    
    print(f"Loaded {n_original:,} records -> Removed {n_dups} duplicates")
    print(f"   Empty-net rows tagged (NOT removed): {n_empty_net} ({n_empty_net/len(df_raw)*100:.1f}%)")
    print(f"   Clean rows: {len(df_raw):,}")
    return df_raw


def aggregate_to_games(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    Collapse shift-level rows into one row per game.
    Engineer winner, league points, and differentials.
    
    Now also aggregates: assists, penalties (for special-teams features).
    
    Returns
    -------
    pd.DataFrame - Game-level DataFrame (`df_games`).
    """
    agg_rules = {
        'home_goals': 'sum', 'away_goals': 'sum',
        'home_xg':    'sum', 'away_xg':    'sum',
        'home_shots': 'sum', 'away_shots': 'sum',
        'went_ot': 'max',
        'home_team': 'first', 'away_team': 'first',
        # NEW: aggregate penalty & assist data for richer features
        'home_assists': 'sum', 'away_assists': 'sum',
        'home_penalties_committed': 'sum', 'away_penalties_committed': 'sum',
        'home_penalty_minutes': 'sum', 'away_penalty_minutes': 'sum',
    }
    df_games = df_raw.groupby('game_id').agg(agg_rules).reset_index()
    
    # Determine winner (no draws should exist ‚Äî OT resolves all ties)
    df_games['winner'] = np.where(
        df_games['home_goals'] > df_games['away_goals'], 'Home', 'Away'
    )
    n_draws = (df_games['home_goals'] == df_games['away_goals']).sum()
    if n_draws > 0:
        print(f"   WARNING: {n_draws} draws found ‚Äî data may be corrupted!")
    
    # League points (vectorized)
    home_win = df_games['home_goals'] > df_games['away_goals']
    away_win = df_games['away_goals'] > df_games['home_goals']
    is_ot    = df_games['went_ot'] == 1
    
    df_games['home_points'] = PTS_REG_LOSS
    df_games.loc[home_win & ~is_ot, 'home_points'] = PTS_REG_WIN
    df_games.loc[home_win &  is_ot, 'home_points'] = PTS_OT_WIN
    df_games.loc[away_win &  is_ot, 'home_points'] = PTS_OT_LOSS
    
    df_games['away_points'] = PTS_REG_LOSS
    df_games.loc[away_win & ~is_ot, 'away_points'] = PTS_REG_WIN
    df_games.loc[away_win &  is_ot, 'away_points'] = PTS_OT_WIN
    df_games.loc[home_win &  is_ot, 'away_points'] = PTS_OT_LOSS
    
    # Differentials
    df_games['goal_diff'] = df_games['home_goals'] - df_games['away_goals']
    df_games['xg_diff']   = df_games['home_xg']    - df_games['away_xg']
    
    print(f"Aggregated into {len(df_games)} unique games (draws: {n_draws}).")
    return df_games

print("Pipeline functions (Step 1) defined.")

============================================================
Cell 7 [code]
============================================================
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  Step 2: Game-level -> Team-perspective Long Format
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def create_long_format(df_games: pd.DataFrame) -> pd.DataFrame:
    """
    Melt game-level data into two rows per game (home + away team perspective).
    
    Adds `is_home` indicator and standardizes column names to
    team-agnostic labels: Team, Opponent, GF, GA, xGF, xGA, SF, SA.
    Now also includes: Assists, PenaltiesFor (own), PenaltiesAgainst (opponent's).
    
    Returns
    -------
    pd.DataFrame - Long-format team stats (`df_schedule_long`).
    """
    cols_to_keep = ['game_id', 'Team', 'Opponent', 'GF', 'GA',
                    'xGF', 'xGA', 'SF', 'SA', 'is_home', 'Result', 'went_ot',
                    'Assists', 'PIM', 'Opp_PIM']
    
    # Home perspective
    df_home = df_games.rename(columns={
        'home_team': 'Team',   'away_team': 'Opponent',
        'home_goals': 'GF',    'away_goals': 'GA',
        'home_xg': 'xGF',     'away_xg': 'xGA',
        'home_shots': 'SF',   'away_shots': 'SA',
        'home_points': 'Result',
        'home_assists': 'Assists',
        'home_penalty_minutes': 'PIM',
        'away_penalty_minutes': 'Opp_PIM',
    }).assign(is_home=1)[cols_to_keep]
    
    # Away perspective
    df_away = df_games.rename(columns={
        'away_team': 'Team',   'home_team': 'Opponent',
        'away_goals': 'GF',    'home_goals': 'GA',
        'away_xg': 'xGF',     'home_xg': 'xGA',
        'away_shots': 'SF',   'home_shots': 'SA',
        'away_points': 'Result',
        'away_assists': 'Assists',
        'away_penalty_minutes': 'PIM',
        'home_penalty_minutes': 'Opp_PIM',
    }).assign(is_home=0)[cols_to_keep]
    
    df_long = pd.concat([df_home, df_away], ignore_index=True)
    df_long = df_long.sort_values(['game_id', 'is_home'], ascending=[True, False]).reset_index(drop=True)
    
    print(f"Long format: {len(df_games)} games -> {len(df_long)} team-game rows")
    return df_long

print("Pipeline functions (Step 2) defined.")

============================================================
Cell 8 [code]
============================================================
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  Step 3: Calculate Multipliers & Performance Scores
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def calculate_multipliers(df_long: pd.DataFrame) -> pd.DataFrame:
    """
    Add game-level weighting columns used during Elo updates.
    
    - **mov_multiplier**: min(ln(|GF-GA| + 1), MOV_CAP) - capped diminishing returns.
    - **xg_share**: xGF / (xGF + xGA) - process quality independent of luck.
    
    NOTE: Compared to previous version:
    - Removed `goal_share` and nested `perf_score` (was triple-nesting weights)
    - xg_share is passed directly to Elo update, combined with result there
    - MoV is now capped at MOV_CAP to prevent blowout dominance
    
    Returns
    -------
    pd.DataFrame - Same DataFrame with new columns appended.
    """
    df = df_long.copy()
    
    # Margin-of-Victory multiplier (CAPPED)
    raw_mov = np.log(np.abs(df['GF'] - df['GA']) + MOV_LOG_BASE)
    df['mov_multiplier'] = np.minimum(raw_mov, MOV_CAP)
    
    # xG share (handle zero-xG edge cases -> default 0.5)
    total_xg = df['xGF'] + df['xGA']
    df['xg_share'] = np.where(total_xg > 0, df['xGF'] / total_xg, 0.5)
    
    # Discipline metric: penalty minute differential (for analysis)
    df['pim_diff'] = df['Opp_PIM'] - df['PIM']  # positive = opponent more penalized = good
    
    print(f"Multipliers computed. Stats:")
    print(df[['mov_multiplier', 'xg_share', 'pim_diff']].describe().round(4))
    
    return df

print("Pipeline functions (Step 3) defined.")

============================================================
Cell 9 [markdown]
============================================================
### Execute the Pipeline

============================================================
Cell 10 [code]
============================================================
# --- Run Steps 1 -> 3 ---

df_raw   = load_and_clean(DATA_PATH)
df_games = aggregate_to_games(df_raw)
df_schedule_long = create_long_format(df_games)
df_schedule_long = calculate_multipliers(df_schedule_long)

# --- Quality Gate ---
assert df_games['game_id'].is_unique, "CRITICAL: Duplicate game IDs!"
assert len(df_schedule_long) == 2 * len(df_games), "Row count mismatch!"
assert (df_games['home_goals'] == df_games['away_goals']).sum() == 0, \
    "CRITICAL: Draws found! OT should resolve all ties."

h_mean = df_games['home_goals'].mean()
a_mean = df_games['away_goals'].mean()
hw_rate = (df_games['home_goals'] > df_games['away_goals']).mean()
ot_rate = df_games['went_ot'].mean()

print(f"\n--- Data Summary ---")
print(f"   Games: {len(df_games)} | Teams: {df_schedule_long['Team'].nunique()}")
print(f"   Home win rate: {hw_rate:.1%} (HFA config: {HOME_ADVANTAGE} Elo pts)")
print(f"   OT rate: {ot_rate:.1%}")
print(f"   Home goals: {h_mean:.2f}/game | Away goals: {a_mean:.2f}/game | Advantage: +{h_mean - a_mean:.3f}")

# Sanity: verify HFA config is reasonable for observed home win rate
hfa_empirical = -400 * np.log10(1/hw_rate - 1)
print(f"   Empirical HFA from data: {hfa_empirical:.1f} Elo pts (configured: {HOME_ADVANTAGE})")
if abs(HOME_ADVANTAGE - hfa_empirical) > 20:
    print(f"   WARNING: HFA config differs significantly from empirical value!")

print("\nPipeline complete. Preview:")
df_schedule_long.head(6)

============================================================
Cell 11 [markdown]
============================================================
---
## 4. Analysis: Pythagorean Expectation (Static Baseline)

Bill James' **Pythagorean expectation** estimates a team's "deserved" win%
from total goals scored vs. allowed:

$$\text{Pyth Win\%} = \frac{GF^{e}}{GF^{e} + GA^{e}}$$

where $e \approx 2.15$ for hockey. Teams whose actual win% significantly
exceeds their Pythagorean expectation may be "lucky" ‚Äî candidates for regression.

============================================================
Cell 12 [code]
============================================================
def compute_pythagorean(df_long: pd.DataFrame) -> pd.DataFrame:
    """
    Build Pythagorean expectation table with actual vs expected win%.
    
    Returns
    -------
    pd.DataFrame ‚Äì One row per team with Pyth_Win%, Actual_Win%, etc.
    """
    # Aggregate season totals
    season_totals = df_long.groupby('Team').agg(
        GF=('GF', 'sum'),
        GA=('GA', 'sum'),
        GP=('game_id', 'nunique'),
        Wins=('Result', lambda x: ((x >= 2).sum()))  # OT Win + Reg Win
    ).reset_index()
    
    # Pythagorean expectation
    gf_exp = season_totals['GF'] ** PYTH_EXPONENT
    ga_exp = season_totals['GA'] ** PYTH_EXPONENT
    season_totals['Pyth_Win%']   = gf_exp / (gf_exp + ga_exp)
    season_totals['Actual_Win%'] = season_totals['Wins'] / season_totals['GP']
    season_totals['Luck_Delta']  = season_totals['Actual_Win%'] - season_totals['Pyth_Win%']
    
    return season_totals.sort_values('Pyth_Win%', ascending=False).reset_index(drop=True)


df_pythagorean = compute_pythagorean(df_schedule_long)

print("Top 10 Teams by Pythagorean Expectation:")
display(df_pythagorean[['Team', 'GP', 'GF', 'GA', 'Pyth_Win%', 'Actual_Win%', 'Luck_Delta']].head(10))

# ‚îÄ‚îÄ‚îÄ Visualization: Actual vs Pythagorean Win% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
fig, ax = plt.subplots(figsize=(8, 6))
sns.scatterplot(data=df_pythagorean, x='Pyth_Win%', y='Actual_Win%', ax=ax, s=60, edgecolor='black', alpha=0.8)
ax.plot([0, 1], [0, 1], 'r--', alpha=0.6, label='y = x (perfect calibration)')
ax.set_title('Actual Win% vs Pythagorean Expectation', fontsize=14)
ax.set_xlabel('Pythagorean Expectation')
ax.set_ylabel('Actual Win%')
ax.legend()
ax.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

# Annotate potential outliers
lucky_teams = df_pythagorean[df_pythagorean['Luck_Delta'] > 0.10]
if not lucky_teams.empty:
    print("\n‚ö†Ô∏è  Teams significantly outperforming Pythagorean (possible regression candidates):")
    for _, row in lucky_teams.iterrows():
        print(f"   {row['Team']}: Actual={row['Actual_Win%']:.3f}, Pyth={row['Pyth_Win%']:.3f}, Œî={row['Luck_Delta']:+.3f}")

============================================================
Cell 13 [markdown]
============================================================
---
## 5. Modeling: Iterative Elo Simulation (Step 4)

**Monte Carlo Bagging**: Run N simulations with shuffled game order.
Each simulation yields one set of final Elo ratings. Averaging across
simulations reduces the bias from any particular game sequence,
producing more robust power rankings.

============================================================
Cell 14 [code]
============================================================
def prepare_simulation_data(df_long: pd.DataFrame) -> list:
    """
    Pre-compute a flat list of game dictionaries for fast iteration.
    Each dict contains everything needed for one Elo update.
    
    Returns
    -------
    list[dict] - One dict per game with keys: home, away, home_pts,
                 xg_share_home, xg_share_away, mov_mult.
    """
    df_home_view = df_long[df_long['is_home'] == 1].set_index('game_id')
    df_away_view = df_long[df_long['is_home'] == 0].set_index('game_id')
    
    # Join on game_id to get both teams' data in one row
    df_joined = df_home_view[['Team', 'Result', 'xg_share', 'mov_multiplier']].join(
        df_away_view[['Team', 'xg_share']],
        lsuffix='_home', rsuffix='_away'
    )
    
    games_for_sim = []
    for _, row in df_joined.iterrows():
        games_for_sim.append({
            'home':           row['Team_home'],
            'away':           row['Team_away'],
            'home_pts':       row['Result'],
            'xg_share_home':  row['xg_share_home'],
            'xg_share_away':  row['xg_share_away'],
            'mov_mult':       row['mov_multiplier']
        })
    
    return games_for_sim


def run_elo_simulation(games_for_sim: list, teams: list, n_sims: int) -> pd.DataFrame:
    """
    Monte Carlo Elo simulation with bagging.
    
    In each iteration:
      1. Initialize all teams at ELO_BASE (1500).
      2. Shuffle the game order (removes temporal bias).
      3. Process each game through the Elo update.
      4. Record final ratings.
    
    Aggregating across N runs gives Mean and Std of each team's Elo,
    naturally producing confidence intervals.
    
    Parameters
    ----------
    games_for_sim : list[dict] - Pre-processed game data.
    teams         : list       - All unique team names.
    n_sims        : int        - Number of Monte Carlo iterations.
    
    Returns
    -------
    pd.DataFrame - Columns: Team, Elo_Mean, Elo_Std, Elo_Rank.
    """
    # Storage: {team_name: [rating_sim1, rating_sim2, ...]}
    rating_distributions = {team: [] for team in teams}
    
    for _ in tqdm(range(n_sims), desc=f'Elo Simulation ({n_sims} runs)'):
        # Initialize
        current_ratings = {team: ELO_BASE for team in teams}
        
        # Shuffle game order
        shuffled_games = games_for_sim.copy()
        random.shuffle(shuffled_games)
        
        # Process season
        for g in shuffled_games:
            new_home, new_away = update_elo_ratings(
                current_ratings[g['home']],
                current_ratings[g['away']],
                g['home_pts'],
                g['xg_share_home'],
                g['xg_share_away'],
                g['mov_mult']
            )
            current_ratings[g['home']] = new_home
            current_ratings[g['away']] = new_away
        
        # Record final ratings
        for team in teams:
            rating_distributions[team].append(current_ratings[team])
    
    # Aggregate results
    elo_summary = []
    for team, ratings in rating_distributions.items():
        elo_summary.append({
            'Team':     team,
            'Elo_Mean': np.mean(ratings),
            'Elo_Std':  np.std(ratings)
        })
    
    df_elo = pd.DataFrame(elo_summary).sort_values('Elo_Mean', ascending=False).reset_index(drop=True)
    df_elo['Elo_Rank'] = df_elo.index + 1
    
    return df_elo

print("Simulation functions defined.")

============================================================
Cell 15 [code]
============================================================
# ‚îÄ‚îÄ‚îÄ Execute Simulation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

games_for_sim = prepare_simulation_data(df_schedule_long)
all_teams     = df_schedule_long['Team'].unique().tolist()

print(f"Prepared {len(games_for_sim)} games √ó {len(all_teams)} teams")
print(f"Running {N_SIMULATIONS} Monte Carlo iterations...\n")

df_elo_rankings = run_elo_simulation(games_for_sim, all_teams, N_SIMULATIONS)

# Add 95% confidence intervals
df_elo_rankings['CI_95_Lower'] = df_elo_rankings['Elo_Mean'] - 1.96 * df_elo_rankings['Elo_Std']
df_elo_rankings['CI_95_Upper'] = df_elo_rankings['Elo_Mean'] + 1.96 * df_elo_rankings['Elo_Std']

print("\nüèÜ Power Rankings (All Teams):")
display(df_elo_rankings[['Elo_Rank', 'Team', 'Elo_Mean', 'Elo_Std', 'CI_95_Lower', 'CI_95_Upper']])

============================================================
Cell 16 [code]
============================================================
# ‚îÄ‚îÄ‚îÄ Visualization: Elo Distribution ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# 1. Histogram of mean Elo ratings
sns.histplot(df_elo_rankings['Elo_Mean'], kde=True, bins=20, ax=axes[0], color='steelblue')
axes[0].set_title(f'Distribution of Mean Elo Ratings ({N_SIMULATIONS} sims)', fontsize=12)
axes[0].set_xlabel('Elo Rating')
axes[0].axvline(ELO_BASE, color='red', linestyle='--', alpha=0.5, label=f'Baseline ({ELO_BASE})')
axes[0].legend()

# 2. Elo vs League Points (correlation validation)
df_league_points = df_schedule_long.groupby('Team')['Result'].sum().reset_index(name='League_Points')
df_validation = df_elo_rankings.merge(df_league_points, on='Team')
corr_elo_pts = df_validation['Elo_Mean'].corr(df_validation['League_Points'])

sns.scatterplot(data=df_validation, x='League_Points', y='Elo_Mean', ax=axes[1], s=60, edgecolor='black')
axes[1].set_title(f'Elo vs League Points (r = {corr_elo_pts:.3f})', fontsize=12)
axes[1].set_xlabel('Total League Points')
axes[1].set_ylabel('Mean Elo Rating')
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(f"\nüìà Elo‚ÄìPoints Pearson Correlation: {corr_elo_pts:.4f}")
if corr_elo_pts > 0.9:
    print("   Strong correlation ‚Üí Elo model captures league standings well.")
elif corr_elo_pts > 0.7:
    print("   Moderate correlation ‚Üí Elo adds nuance beyond raw points.")
else:
    print("   ‚ö†Ô∏è  Weak correlation ‚Üí Review model parameters.")

============================================================
Cell 17 [markdown]
============================================================
---
## 6. Cross-Validation: Evaluate Predictive Quality

Before making predictions, we evaluate how well the Elo model **actually predicts**
game outcomes on training data. This uses leave-one-out style evaluation:
for each game, we compute P(home win) from the current Elo state,
then measure log loss against the actual outcome.

This helps us verify:
1. The model is better than random guessing (log loss < 0.693)
2. The HFA parameter is well-calibrated
3. The overall calibration of predicted probabilities

============================================================
Cell 18 [code]
============================================================
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
#  Cross-Validation: Predictive Log Loss
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

def evaluate_model_logloss(games_for_sim: list, teams: list, 
                            k: float, hfa: float, n_sims: int = 20) -> float:
    """
    Run Monte Carlo Elo and compute average log loss on training data.
    
    For each simulation, during the season replay, we record P(home win)
    BEFORE updating Elo (out-of-sample prediction) and compare to actual result.
    """
    all_log_losses = []
    
    for _ in range(n_sims):
        current_ratings = {team: ELO_BASE for team in teams}
        shuffled = games_for_sim.copy()
        random.shuffle(shuffled)
        
        preds = []
        actuals = []
        
        for g in shuffled:
            r_home = current_ratings[g['home']]
            r_away = current_ratings[g['away']]
            
            # Predict BEFORE update
            p_home = elo_expected_score(r_home, r_away, hfa=hfa)
            preds.append(np.clip(p_home, 1e-6, 1 - 1e-6))
            actuals.append(1 if g['home_pts'] >= 2 else 0)  # Win = pts >= 2
            
            # Then update
            pts_map = {3: 0, 2: 1, 1: 2, 0: 3}
            result_pts_away = pts_map.get(g['home_pts'], 0)
            
            s_home = composite_actual_score(g['home_pts'], g['xg_share_home'])
            s_away = composite_actual_score(result_pts_away, g['xg_share_away'])
            total = s_home + s_away
            if total == 0:
                s_home, s_away = 0.5, 0.5
            else:
                s_home /= total
                s_away /= total
            
            exp_home = elo_expected_score(r_home, r_away, hfa=hfa)
            mov = min(np.log(abs(g['mov_mult']) + 1), MOV_CAP) if g['mov_mult'] == 0 else g['mov_mult']
            
            current_ratings[g['home']] += k * g['mov_mult'] * (s_home - exp_home)
            current_ratings[g['away']] += k * g['mov_mult'] * (s_away - (1.0 - exp_home))
        
        all_log_losses.append(log_loss(actuals, preds))
    
    return np.mean(all_log_losses)


# Quick parameter sensitivity check
print("--- Parameter Sensitivity (Log Loss, lower = better) ---")
print(f"{'K':>4} | {'HFA':>5} | {'Log Loss':>10}")
print("-" * 28)

test_configs = [
    (K_FACTOR, HOME_ADVANTAGE, "CURRENT"),
    (15, 50, ""),
    (20, 45, ""),
    (20, 50, ""),
    (25, 50, ""),
    (20, 55, ""),
    (30, 50, ""),
]

best_ll = float('inf')
best_cfg = None
for k, hfa, label in test_configs:
    ll = evaluate_model_logloss(games_for_sim, all_teams, k=k, hfa=hfa, n_sims=10)
    marker = f" <- {label}" if label else ""
    if ll < best_ll:
        best_ll = ll
        best_cfg = (k, hfa)
        if not label:
            marker = " <- BEST"
    print(f"{k:4d} | {hfa:5d} | {ll:.5f}{marker}")

print(f"\nBest config: K={best_cfg[0]}, HFA={best_cfg[1]} (LL={best_ll:.5f})")
print(f"Baseline (random): 0.69315")
print(f"Improvement over random: {(0.69315 - best_ll)/0.69315*100:.1f}%")

============================================================
Cell 19 [markdown]
============================================================
---
## 7. Prediction & Output (Step 5)

Apply Elo ratings to predict Round 1 matchup outcomes.

============================================================
Cell 20 [code]
============================================================
def predict_matchups(df_elo: pd.DataFrame, matchup_path: str) -> pd.DataFrame:
    """
    Load matchups, merge Elo ratings, and compute win probabilities.
    
    Prediction Formula (same logistic as the Elo engine):
        P(Home Win) = 1 / (1 + 10^((Elo_Away - (Elo_Home + HFA)) / 400))
    
    Parameters
    ----------
    df_elo       : pd.DataFrame - Elo rankings with 'Team' and 'Elo_Mean'.
    matchup_path : str          - Path to matchups CSV.
    
    Returns
    -------
    pd.DataFrame - Predictions with columns: Home, Away, Elo_Home, Elo_Away,
                   Prob_Home, Predicted_Winner, Risk_Level.
    """
    # Load and standardize column names
    df_matchups = pd.read_csv(matchup_path)
    
    if 'home_team' in df_matchups.columns and 'away_team' in df_matchups.columns:
        df_pred = df_matchups[['home_team', 'away_team']].copy()
        df_pred.columns = ['Home', 'Away']
    else:
        raise ValueError(f"Expected 'home_team' and 'away_team' columns. Found: {df_matchups.columns.tolist()}")
    
    # Merge Elo ratings
    df_pred = df_pred.merge(
        df_elo[['Team', 'Elo_Mean', 'Elo_Std']], left_on='Home', right_on='Team', how='left'
    ).rename(columns={'Elo_Mean': 'Elo_Home', 'Elo_Std': 'Elo_Std_Home'}).drop('Team', axis=1)
    
    df_pred = df_pred.merge(
        df_elo[['Team', 'Elo_Mean', 'Elo_Std']], left_on='Away', right_on='Team', how='left'
    ).rename(columns={'Elo_Mean': 'Elo_Away', 'Elo_Std': 'Elo_Std_Away'}).drop('Team', axis=1)
    
    # Check for missing teams
    missing = df_pred[df_pred[['Elo_Home', 'Elo_Away']].isnull().any(axis=1)]
    if not missing.empty:
        print("WARNING: Unmatched teams (defaulting to 1500):")
        display(missing)
        df_pred['Elo_Home'].fillna(ELO_BASE, inplace=True)
        df_pred['Elo_Away'].fillna(ELO_BASE, inplace=True)
    else:
        print("All matchup teams found in Elo ratings.")
    
    # Win probability (reuses the same logistic formula)
    df_pred['Prob_Home'] = df_pred.apply(
        lambda r: elo_expected_score(r['Elo_Home'], r['Elo_Away'], hfa=HOME_ADVANTAGE),
        axis=1
    )
    
    # Also compute probability WITHOUT HFA for transparency
    df_pred['Prob_Home_NoHFA'] = df_pred.apply(
        lambda r: elo_expected_score(r['Elo_Home'], r['Elo_Away'], hfa=0),
        axis=1
    )
    
    # Elo difference (pure strength gap, no HFA)
    df_pred['Elo_Diff'] = df_pred['Elo_Home'] - df_pred['Elo_Away']
    
    # Predicted winner and risk classification
    df_pred['Predicted_Winner'] = np.where(
        df_pred['Prob_Home'] > 0.5, df_pred['Home'], df_pred['Away']
    )
    df_pred['Prob_Winner'] = np.where(
        df_pred['Prob_Home'] > 0.5, df_pred['Prob_Home'], 1 - df_pred['Prob_Home']
    )
    df_pred['Risk_Level'] = df_pred['Prob_Home'].apply(classify_risk)
    
    return df_pred


# --- Execute Prediction ---
df_submission = predict_matchups(df_elo_rankings, MATCHUP_PATH)

print("\nPrediction Table:")
display(df_submission[['Home', 'Away', 'Elo_Home', 'Elo_Away', 'Elo_Diff',
                        'Prob_Home_NoHFA', 'Prob_Home', 'Predicted_Winner', 'Risk_Level']])

# Summary stats
n_home = (df_submission['Prob_Home'] > 0.5).sum()
n_away = (df_submission['Prob_Home'] <= 0.5).sum()
print(f"\nHome wins predicted: {n_home} | Away wins predicted: {n_away}")
print(f"Close games (|P-0.5| < 0.05): {(df_submission['Prob_Home'].between(0.45, 0.55)).sum()}")
print(f"Avg home win prob: {df_submission['Prob_Home'].mean():.3f}")

============================================================
Cell 21 [code]
============================================================
# ‚îÄ‚îÄ‚îÄ PDO Sustainability Check ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#
# PDO = Shooting% + Save% (should regress to ~1.0 over time).
# A high-Elo team with PDO > 1.04 may be benefiting from
# unsustainable luck rather than true skill.

def compute_pdo(df_long: pd.DataFrame, df_elo: pd.DataFrame) -> pd.DataFrame:
    """
    Calculate PDO for each team and flag unsustainable performers.
    """
    pdo_stats = df_long.groupby('Team').agg(
        GF=('GF', 'sum'), SF=('SF', 'sum'),
        GA=('GA', 'sum'), SA=('SA', 'sum')
    ).reset_index()
    
    # Avoid division by zero
    pdo_stats['SF'] = pdo_stats['SF'].replace(0, 1)
    pdo_stats['SA'] = pdo_stats['SA'].replace(0, 1)
    
    pdo_stats['Sh_Pct'] = pdo_stats['GF'] / pdo_stats['SF']      # Shooting %
    pdo_stats['Sv_Pct'] = 1 - (pdo_stats['GA'] / pdo_stats['SA']) # Save %
    pdo_stats['PDO']    = pdo_stats['Sh_Pct'] + pdo_stats['Sv_Pct']
    
    return df_elo.merge(pdo_stats[['Team', 'PDO', 'Sh_Pct', 'Sv_Pct']], on='Team')


df_pdo_check = compute_pdo(df_schedule_long, df_elo_rankings)

print("\nüîç Regression Analysis (PDO Luck Check):")
print(f"   Checking top-{PDO_TOP_N} Elo teams for PDO > {PDO_THRESHOLD}...\n")

lucky_top_teams = df_pdo_check[
    (df_pdo_check['Elo_Rank'] <= PDO_TOP_N) &
    (df_pdo_check['PDO'] > PDO_THRESHOLD)
]

if not lucky_top_teams.empty:
    for _, row in lucky_top_teams.iterrows():
        print(f"   ‚ö†Ô∏è  {row['Team']} (Rank {row['Elo_Rank']}) ‚Äî PDO={row['PDO']:.3f} "
              f"(Sh%={row['Sh_Pct']:.3f}, Sv%={row['Sv_Pct']:.3f}). Expect regression.")
else:
    print("   ‚úÖ Top teams have sustainable underlying metrics (PDO ‚â§ 1.04).")

============================================================
Cell 22 [code]
============================================================
# ‚îÄ‚îÄ‚îÄ Visualization: Matchup Odds Spread ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

fig, ax = plt.subplots(figsize=(10, max(6, len(df_submission) * 0.4)))

df_sorted = df_submission.sort_values('Prob_Home')
y_labels  = df_sorted['Home'] + ' vs ' + df_sorted['Away']
advantage = df_sorted['Prob_Home'] - 0.5
colors    = ['#2ecc71' if x > 0 else '#e74c3c' for x in advantage]

ax.barh(y_labels, advantage, color=colors, edgecolor='white', height=0.6)
ax.axvline(0, color='black', linewidth=0.8)
ax.set_title('Matchup Advantage Spread', fontsize=14)
ax.set_xlabel('‚Üê Away Favored    |    Home Favored ‚Üí')
ax.grid(axis='x', alpha=0.3)

# Annotate probabilities
for i, (adv, prob) in enumerate(zip(advantage, df_sorted['Prob_Home'])):
    ax.text(adv + (0.01 if adv >= 0 else -0.01), i,
            f'{prob:.1%}', va='center', ha='left' if adv >= 0 else 'right', fontsize=8)

plt.tight_layout()
plt.show()

============================================================
Cell 23 [code]
============================================================
# ‚îÄ‚îÄ‚îÄ Export Submission ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

df_submission.to_csv(SUBMISSION_PATH, index=False)
print(f"\nüíæ Predictions saved to: {SUBMISSION_PATH}")
print(f"   {len(df_submission)} matchups predicted.")
print(f"\n{'‚îÄ'*60}")
print(f"  Home wins predicted: {(df_submission['Prob_Home'] > 0.5).sum()}")
print(f"  Away wins predicted: {(df_submission['Prob_Home'] <= 0.5).sum()}")
print(f"  Avg home win prob:   {df_submission['Prob_Home'].mean():.3f}")
print(f"{'‚îÄ'*60}")
print("\n‚úÖ Phase 1a Pipeline Complete.")

============================================================
Cell 24 [code]
============================================================


